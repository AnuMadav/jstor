---
title: "Automating File Import"
author: "Thomas Klebel"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Intro
The `find_*` functions from `jstor` all work on a single file. Data from DfR
however contains many single files, from up to 25,000 when using the self-service
functions, up to several hundreds of thousands of files when requesting a large
dataset via an agreement.

In this vignette I will introduce _one_ way to deal with this large amount of
files. I start with how to list all corresponding files and follow up with some
utility functions implemented in `jstor`. Possibly there are other ways one could
import all those files, but this is how I did it four our research.


# Unzip containers
Data from DfR comes in `*.zip`-files. For simple purposes it might be sensible
to unzip to a temporary directory (with `temp()` and `unzip()`) but for my 
research I simply extracted files to an external SSD, since I a) lacked disk space,
b) needed to read them fast, and c) wanted to be able to look at specific files for
debugging.

# List files
There are many ways to generate a list of all files: 
`list.files()` or using `system()` in conjunction with `find` on unix-like systems
are common options. 

For demonstration purposes I use files contained in `jstor` which can be accessed
via `system.file`:
```{r, echo=TRUE}
example_dir <- system.file("extdata", package = "jstor")
```

## `list.files`

```{r}
file_names_listed <- list.files(path = example_dir, full.names = T, pattern = "*.xml")
file_names_listed
```

## `system` and `find`
```{r, eval=FALSE}
file_names <- system(paste0("cd ", example_dir, "; find . -name '*.xml' -type f"), intern = TRUE)
```

```{r, eval=FALSE}
library(stringr)

file_names_system <- file_names %>%
  str_replace("^\\.\\/", "") %>%
  str_c(example_dir, "/", .)

file_names_system
#> [1] "/Library/Frameworks/R.framework/Versions/3.4/Resources/library/jstor/extdata/sample_with_footnotes.xml" 
#> [2] "/Library/Frameworks/R.framework/Versions/3.4/Resources/library/jstor/extdata/sample_book.xml"
#> [3] "/Library/Frameworks/R.framework/Versions/3.4/Resources/library/jstor/extdata/sample_with_references.xml"
```

In this case the two approaches give the same result.
The main difference seems to be though, that `list.files` sorts the output, 
whereas `find` does not. For a large amour of files (200,000) this makes
`list.files` slower, for smaller datasets the difference shouldn't make an
impact.

# Batch import
Once the file list is generated, we can apply any of the `find_*`-functions to
the list. A good and simple way for small to moderate amounts of files is to use
`map_df` from purrr:

```{r, results='asis'}
library(jstor)
library(purrr)
library(stringr)

result <- file_names_listed %>% 
  keep(str_detect, "with") %>% # only operate on articles, remove book sample
  map_df(find_article)

result %>% knitr::kable()
```

This works well if 1) there are no errors and 2) if there is only a moderate
size of files. If you try to import many files, you could either run into
problems regarding RAM, or it would take a long time and would benefit from
executing the process in parallel.

The function `jstor_import()` takes care of errors and uses
`foreach::foreach()` and `snow::makeCluster()` to speed up the process.

The whole approach about importing many files is implemented like this:

- list all files which are to be converted
- split this list of files into batches of reasonable size
- apply on each batch the following process:
    + import and convert data in parallel
    + write the result into a single .csv-file
    + write any errors in a separate .csv-file
    
`jstor_import()` takes up to seven arguments:

- *in_paths*: this should be a character vector of all `xml-`files you want to
import
- *out_file*: the file name for the resulting `.csv-`files. 
- *out_path*: in case you want to export the files to a specific directory, 
specify it here.
- *.f*: the function to use (one of `find_article`, `find_authors`, `find_references`,
or `find_footnotes`)
- *files_per_batch*: how many files should be converted per batch? For my system,
20,000 files per batch worked well.
- *cores* number of cores to use for parallel processing (`1` on windows).
- *show_progress*: if `TRUE` and run from an interactive session, a progress bar
will be displayed for each batch.

You can specify all parameters like this:
```{r, eval=FALSE}
jstor_import(in_paths = vector_of_all_my_files, out_file = "metadata",
             out_path = "path/to/out/dir", .f = find_article,
             files_per_batch = 20000, cores = 4)
```

Suppose you had 100,000 files to convert, and no errors happened, then you should
have in your `"path/to/out/dir"` the following five files:
```{r, echo=FALSE}
paste0("metadata-", 1:5, ".csv")
```

